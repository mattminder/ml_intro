{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scatter(falling_times, c=\"royalblue\"):\n",
    "    plt.scatter(falling_times[\"height\"], falling_times[\"duration\"], c=c)\n",
    "    plt.xlabel(\"height (m)\")\n",
    "    plt.ylabel(\"time (s)\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    \n",
    "def fit_model_with_complexity(data, complexity, degree=3):\n",
    "    \"\"\"Fit a spline to the data - basically a wiggly line that fits as many points as possible.\"\"\"\n",
    "    pipe = make_pipeline(SplineTransformer(complexity, degree), LinearRegression())\n",
    "    pipe.fit(data[[\"height\"]], data[\"duration\"])\n",
    "    return pipe\n",
    "\n",
    "def predict_model(pipe):\n",
    "    interpolation = pd.DataFrame(\n",
    "        {\n",
    "            \"height\": np.linspace(0, 5, 201)\n",
    "        }\n",
    "    )\n",
    "    interpolation[\"prediction\"] = pipe.predict(interpolation)\n",
    "    return interpolation\n",
    "\n",
    "def calculate_error(pipe, data):\n",
    "    predictions = pipe.predict(data[[\"height\"]])\n",
    "    return mean_absolute_error(predictions, data[\"duration\"])\n",
    "\n",
    "def plot_interpolation(data, complexity):\n",
    "    pipe = fit_model_with_complexity(data, complexity)\n",
    "    interpolation = predict_model(pipe)\n",
    "    error = calculate_error(pipe, data)\n",
    "    plt.plot(interpolation[\"height\"], interpolation[\"prediction\"], color=\"darkorange\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.title(f\"Mean Absolute Error: {error:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Sets\n",
    "Now we will look at another important concept: Train and Test Sets. \n",
    "\n",
    "## Falling-Time Calculation\n",
    "\n",
    "Consider again the example of the ball falling time. The data (still) looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_times = pd.read_csv(\"../data/ball_falling_time.csv\")\n",
    "plot_scatter(falling_times)\n",
    "plt.title(\"Falling times of a ball from different heights\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem with our current approach\n",
    "We could achieve a perfect fit by learning by heart all data points (i.e. an error of 0). Such a model may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(falling_times)\n",
    "plot_interpolation(falling_times, 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this model is clearly wrong: If we increase the height, we should *always* have a longer falling time. The orange curve therefore should never go down.\n",
    "\n",
    "**This example shows that the metric we have been calculating until now is not appropriate to measure model quality.**\n",
    "\n",
    "### The Solution: Splitting into Training and Testing Set\n",
    "\n",
    "The solution to this problem is quite easy: Before fitting our model, we split our data set randomly in a **train set** and a **test set**. During model fitting, we will only show the model the data points in the train set. When we evaluate our model by calculating the performance metric, we will do this on the test set. This way, we test the model on data points that it has not seen during training.\n",
    "\n",
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(\n",
    "    falling_times,\n",
    "    train_size=.7,  # we randomly choose 70% of our data points for training, the remaining will be test data\n",
    "    random_state=123,  # set the seed to ensure reproducibility\n",
    ")\n",
    "\n",
    "plot_scatter(train)\n",
    "plot_scatter(test, c=\"silver\")\n",
    "\n",
    "plt.title(\"Data After Splitting\")\n",
    "plt.legend([\"Train Set\", \"Test Set\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we train the same model as before, we will get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpolation_split(train, test, complexity):\n",
    "    pipe = fit_model_with_complexity(train, complexity)\n",
    "    interpolation = predict_model(pipe)\n",
    "    train_error = calculate_error(pipe, train)\n",
    "    test_error = calculate_error(pipe, test)\n",
    "    plt.plot(interpolation[\"height\"], interpolation[\"prediction\"], color=\"darkorange\")\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "    plt.title(f\"Train MAE: {train_error:.3f}, Test MAE: {test_error:.3f}\")\n",
    "\n",
    "\n",
    "plot_scatter(train)\n",
    "plot_scatter(test, c=\"silver\")\n",
    "plot_interpolation_split(train, test, 26)\n",
    "plt.legend([\"Train Set\", \"Test Set\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our model still perfectly learns the blue dots by heart, but performs quite badly on the grey points that it hasn't seen during training. The error on the train set is still 0, but on the test set the model has a mean absolute error of 0.091. \n",
    "\n",
    "This case, where our model performs extremly well on the training set but poorly on the test set is called **overfitting**. Another way to say this is that the model does not **generalize** well - it just reproduces what is has learned by heart.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "If on the other hand we have a model that isn't flexible enough to fit the data, we have **underfitting**. \n",
    "\n",
    "In our toy model, we have a parameter \"complexity\" that controls how flexible our model is. Play with this parameter to see how this affects the model, the train and the test error. For which value of complexity do we get the best fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS (value must be an integer >= 2)\n",
    "complexity = 2\n",
    "\n",
    "# don't change\n",
    "plot_scatter(train)\n",
    "plot_scatter(test, c=\"silver\")\n",
    "plot_interpolation_split(train, test, complexity)\n",
    "plt.legend([\"Train Set\", \"Test Set\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Error vs. Complexity\n",
    "If we keep increasing the complexity and record the train and test errors on our dataset, we obtain the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors for different values of complexity\n",
    "errors = []\n",
    "for complexity in range(2, 12):\n",
    "    pipe = fit_model_with_complexity(train, complexity)\n",
    "\n",
    "    train_error = calculate_error(pipe, train)\n",
    "    test_error = calculate_error(pipe, test)\n",
    "    errors.append(\n",
    "        {\n",
    "            \"complexity\": complexity,\n",
    "            \"train_error\": train_error,\n",
    "            \"test_error\": test_error,\n",
    "        }\n",
    "    )\n",
    "\n",
    "errors_df = pd.DataFrame(errors)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(errors_df[\"complexity\"], errors_df[\"train_error\"])\n",
    "plt.plot(errors_df[\"complexity\"], errors_df[\"test_error\"])\n",
    "plt.axvline(errors_df[\"complexity\"].loc[errors_df[\"test_error\"].argmin()], color=\"k\")\n",
    "\n",
    "plt.legend([\"Train Error\", \"Test Error\", \"Best Model\"])\n",
    "plt.xlabel(\"Complexity\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "\n",
    "plt.title(\"Under- and Overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture above shows the evolution of the train and test errors, depending on the complexity. The best model for our data has complexity 6. All models to the left of the black line (with complexity less than 6) aren't flexible enough in order to fit the data - and are thus *underfitting* the data. All models to the right of the black line (with complexity more than 6) are too flexible and learn the training data by heart. These models are *overfitting* the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Life Expectancy Data\n",
    "\n",
    "The same problem exists on our life expectancy data: A model could just learn by heart all of the data points to get a low training error, but will perform poorly on new data. \n",
    "\n",
    "If we stick to linear regression, we do not have a specific parameter that governs complexity. Instead, the complexity is given by the number of features that we provide to the model: By adding more features, the model becomes more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy = pd.read_csv(\"../data/who_data.csv\", index_col=0)\n",
    "\n",
    "# filter missing values\n",
    "has_missing_values = life_expectancy.isna().sum(axis=1) == 0\n",
    "life_expectancy = life_expectancy[has_missing_values]\n",
    "\n",
    "# create the features that we had\n",
    "life_expectancy[\"log_GDP_per_capita\"] = np.log10(life_expectancy[\"GDP\"] / life_expectancy[\"population\"])\n",
    "life_expectancy[\"log_CHE\"] = np.log10(life_expectancy[\"CHE\"])\n",
    "\n",
    "# split into train and test data\n",
    "train, test = train_test_split(life_expectancy, train_size=.7, random_state=12)\n",
    "\n",
    "# define function that calculates errors for us\n",
    "def fit_and_calculate_errors(train, test, feature_list):\n",
    "    regression = LinearRegression()\n",
    "    regression.fit(train[feature_list], train[\"life_expectancy\"])\n",
    "\n",
    "    train_error = mean_absolute_error(\n",
    "        train[\"life_expectancy\"],\n",
    "        regression.predict(train[feature_list])\n",
    "    )\n",
    "    test_error = mean_absolute_error(\n",
    "        test[\"life_expectancy\"],\n",
    "        regression.predict(test[feature_list])\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model only on the log-transformed GDP per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_calculate_errors(train, test, [\"log_GDP_per_capita\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we perform if we add the log-transformed CHE-data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_calculate_errors(train, test, [\"log_GDP_per_capita\", \"log_CHE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we improved both in the train and the test error. This means that only using the GDP was underfitting the data. Now let's add the prevalence of underweight adults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_calculate_errors(train, test, [\"log_GDP_per_capita\", \"log_CHE\", \"prevalence_underweight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_calculate_errors(train, test, ['population', 'GDP', 'pol3', 'dpt3',\n",
    "       'prevalence_obesity', 'fine_particular_matter', 'hepB',\n",
    "       'CHE', 'prevalence_underweight',\n",
    "       'alcohol_consumption', 'log_GDP_per_capita', 'log_CHE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train error decreased even more, but the test error increased. This means that we are now overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write down a definition for the following terms:\n",
    "\n",
    "- **training set**\n",
    "- **test set**\n",
    "- **underfitting**\n",
    "- **overfitting**\n",
    "- **generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try out the combinations of features that worked best for you in the previous notebook. \n",
    "- Could you improve performance on the test set, or were you overfitting? \n",
    "- Can you find a combination of features that further decreases the test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
